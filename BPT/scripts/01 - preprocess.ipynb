{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45244f74",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199c3b9",
   "metadata": {},
   "source": [
    "**ALPHA** is a constant that is commonly used in statistical hypothesis testing to determine the significance level of a test. The value of ALPHA is typically set to 0.05 (or 5%), which means there is a 5% chance that the test results will be considered significant by chance. The value of ALPHA is used in conjunction with the p-value calculated from the data to determine whether the null hypothesis should be rejected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6080aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c507e",
   "metadata": {},
   "source": [
    "**RANDOM_STATE** is a constant that is typically used to initialize the random number generator in a library or algorithm that uses randomness in some aspect of its operation. This means that when the RANDOM_STATE is fixed to a certain value, the algorithm will generate the same random results every time it is run, which can be useful for reproducing results and ensuring result consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b05727",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4884e1c",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d25510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, shapiro\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776b900",
   "metadata": {},
   "source": [
    "# READ FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb29d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the data file\n",
    "file_path = '../dataset/dataset.xlsx'\n",
    "\n",
    "dtypes = {'Sex': 'category', \n",
    "          'Age': int, \n",
    "          'Height': float, \n",
    "          'Weight': float, \n",
    "          'BMI': float, \n",
    "          'BSA': float,\n",
    "          'DM': 'category', \n",
    "          'HBP': 'category', \n",
    "          'Type of Surgery': 'category', \n",
    "          'Redo': 'category', \n",
    "          'Urgency': 'category', \n",
    "          'CPB': 'category',\n",
    "          'Ht': float, \n",
    "          'Hb': float, \n",
    "          'Creatinine': float, \n",
    "          'Blood Bags': int, \n",
    "          'TRUST': int, \n",
    "          'TRACK': int}\n",
    "\n",
    "# Read the Excel file and select only the desired columns\n",
    "df = pd.read_excel(file_path, \n",
    "                   usecols=list(dtypes.keys()), \n",
    "                   dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008d37d",
   "metadata": {},
   "source": [
    "## - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f206525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset was filtered to include only individuals over the age of 18\n",
    "df = df[df.Age >= 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588492e8",
   "metadata": {},
   "source": [
    "# VISUALIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7df8b",
   "metadata": {},
   "source": [
    "## - Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77a5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e17de",
   "metadata": {},
   "source": [
    "## - Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358085e5",
   "metadata": {},
   "source": [
    "## - Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527287e",
   "metadata": {},
   "source": [
    "# FEATURES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a99d8e",
   "metadata": {},
   "source": [
    "## - Categoric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589014b",
   "metadata": {},
   "source": [
    "- Type of Surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00470b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize string values by converting to lowercase and removing accents\n",
    "df['Type of Surgery'] = df['Type of Surgery'].apply(lambda x: x.strip().lower().replace('[^a-z\\s]', ''))\n",
    "\n",
    "# Create binary columns for each unique type of surgery\n",
    "for type_of_surgery in df['Type of Surgery'].unique():\n",
    "    df['type_' + type_of_surgery] = (df['Type of Surgery'] == type_of_surgery).astype(int)\n",
    "\n",
    "# Drop the original 'Type of Surgery' column\n",
    "df.drop('Type of Surgery',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82eafc",
   "metadata": {},
   "source": [
    "- Replace 2 with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns with a specific value to binary encoding\n",
    "for col in list(df.select_dtypes(include=['category']).columns):\n",
    "    df[col] = df[col].replace({2: 0}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a5ab8",
   "metadata": {},
   "source": [
    "# DATA DIVISION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc403b8d",
   "metadata": {},
   "source": [
    "The importance of dividing data into training and testing subsets lies in the ability to assess a machine learning model's performance on unseen data. By using a portion of the available data for training, we can build a model that learns the underlying patterns in the data. Then, by evaluating the model's performance on a separate set of testing data, we can obtain an estimate of its ability to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into predictor and target variables\n",
    "X = df.drop(columns=['Blood Bags'],axis=1) \n",
    "y = df['Blood Bags'] >= 1\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06555a5",
   "metadata": {},
   "source": [
    "# NULL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36473a",
   "metadata": {},
   "source": [
    "Imputation of missing values involves replacing missing values with estimated values based on a variety of statistical or machine learning techniques. The goal is to replace missing values with plausible and relevant values that allow the data to be analyzed and used for modeling or analysis purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06807c",
   "metadata": {},
   "source": [
    "## - Hb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d6a49",
   "metadata": {},
   "source": [
    "We will examine the correlation between the 'Hb' feature and other features to explore the potential of using any feature to estimate missing values in 'Hb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "correlation = X_train.corr()['Hb']\n",
    "\n",
    "# Create a bar plot of the correlations\n",
    "correlation.plot(kind='bar')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title('Correlation with column \"Hb\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78dbda9",
   "metadata": {},
   "source": [
    "Due to a strong correlation between 'Hb' and 'Ht', we will use 'Ht' values to estimate missing 'Hb' values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a722e",
   "metadata": {},
   "source": [
    "- Impute missing 'Hb' values in the X_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values in 'Hb'\n",
    "missing_hb = np.isnan(X_train['Hb'])\n",
    "\n",
    "# Iterate over rows with missing 'Hb' values\n",
    "for index, patient in X_train[missing_hb].iterrows():\n",
    "    # Find the indices of the three closest 'Ht' values\n",
    "    indices = (X_train.loc[~missing_hb, 'Ht'] - patient['Ht']).abs().sort_values().index[:3]\n",
    "    # Estimate 'Hb' by taking the mean of the closest 'Hb' values\n",
    "    X_train.loc[index, 'Hb'] = X_train.loc[indices, 'Hb'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58021c3b",
   "metadata": {},
   "source": [
    "- Impute missing 'Hb' values in the X_test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values in 'Hb'\n",
    "missing_hb = np.isnan(X_test['Hb'])\n",
    "\n",
    "# Iterate over rows with missing 'Hb' values\n",
    "for index, patient in X_test[missing_hb].iterrows():\n",
    "    # Find the indices of the three closest 'Ht' values\n",
    "    indices = (X_train['Ht'] - patient['Ht']).abs().sort_values().index[:3]\n",
    "    # Estimate 'Hb' by taking the mean of the closest 'Hb' values\n",
    "    X_test.loc[index, 'Hb'] = X_train.loc[indices, 'Hb'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e6998",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22583f",
   "metadata": {},
   "source": [
    "## - Statistical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cca39",
   "metadata": {},
   "source": [
    "In this analysis, we will be performing feature selection based on statistical differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a36841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_of_binary_feature(feature):\n",
    "    \n",
    "    # Merge the data into a single DataFrame\n",
    "    data = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency_table = pd.crosstab(data[feature], data['Blood Bags'])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    percentages = contingency_table.div(contingency_table.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plot the contingency table\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(contingency_table, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "    plt.xlabel('Blood Bags')\n",
    "    plt.ylabel(feature)\n",
    "    plt.title('Contingency Table')\n",
    "    plt.show()\n",
    "\n",
    "    # Print counts and percentages\n",
    "    for index, row in contingency_table.iterrows():\n",
    "        total_samples = row.sum()\n",
    "        positive_percentage = 100 * row[True] / total_samples\n",
    "        negative_percentage = 100 * row[False] / total_samples\n",
    "        \n",
    "        print(f\"{feature} == {index}: {total_samples} ({100*total_samples/len(X_train):.2f}%)\")\n",
    "        print(f\" - positives: {row[True]} ({positive_percentage:.2f}%)\")\n",
    "        print(f\" - negatives: {row[False]} ({negative_percentage:.2f}%)\")\n",
    "    \n",
    "    # Perform the Chi-square test\n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    print(\"\\nChi-square test:\")\n",
    "    print(\"Chi-square statistic:\", chi2_stat)\n",
    "    print(\"p-value:\", p_value, '\\n')\n",
    "    \n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ce906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_of_numerical_feature(feature, test='auto'):\n",
    "    \n",
    "    # Merge the data into a single DataFrame\n",
    "    data = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # Descriptive statistics\n",
    "    print(data.groupby('Blood Bags')[feature].describe())\n",
    "\n",
    "    # Density plot\n",
    "    data.groupby('Blood Bags')[feature].plot(kind='density',legend=True)\n",
    "    plt.show()\n",
    "        \n",
    "    # grouping\n",
    "    group1 = data[data['Blood Bags'] == 0][feature]\n",
    "    group2 = data[data['Blood Bags'] == 1][feature]\n",
    "    \n",
    "    # Check normality\n",
    "    if test == 'auto':\n",
    "        _, p_value_group1 = shapiro(group1)\n",
    "        _, p_value_group2 = shapiro(group2)\n",
    "\n",
    "        if p_value_group1 > 0.05 and p_value_group2 > 0.05:\n",
    "            test = 't-test'\n",
    "        else:\n",
    "            test = 'U test'\n",
    "    \n",
    "    # Mann-Whitney U test\n",
    "    if test == 'U test':\n",
    "        U_stat, p_value = mannwhitneyu(group1, group2)\n",
    "        print(\"Mann-Whitney U test:\")\n",
    "        print(\"U statistic:\", U_stat)\n",
    "        print(\"p-value:\", p_value, '\\n')\n",
    "        return p_value\n",
    "      \n",
    "    # Student's t-test\n",
    "    elif test == 't-test':  \n",
    "        t_stat, p_value = ttest_ind(group1, group2)\n",
    "        print(\"Student's t-test:\")\n",
    "        print(\"t-statistic:\", t_stat)\n",
    "        print(\"p-value:\", p_value, '\\n')\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b065f6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in X_train.columns:\n",
    "    \n",
    "    print('-'*(33-len(feature)//2), feature.upper(), '-'*(33-len(feature)//2))\n",
    "    \n",
    "    if len(X_train[feature].unique()) == 2:\n",
    "        p = analysis_of_binary_feature(feature)\n",
    "    else:\n",
    "        p = analysis_of_numerical_feature(feature)\n",
    "    \n",
    "    if p < ALPHA:\n",
    "        FEATURES.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a73f5d",
   "metadata": {},
   "source": [
    "## - Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa8007",
   "metadata": {},
   "source": [
    "Highly correlated features can provide redundant information and may not add much value to the model. In such cases, it is possible to consider removing one of the features to avoid multicollinearity and reduce the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = pd.concat([X_train[FEATURES], y_train], axis=1).corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, cmap=cmap, center=0, annot=True, square=True, fmt='.2f',\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5}, annot_kws={\"fontsize\":12})\n",
    "\n",
    "# Customize tick labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='right', fontsize=12)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=12)\n",
    "\n",
    "# Add a title\n",
    "ax.set_title(\"Correlation Matrix\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES.remove('Weight') # Highly correlated with BSA\n",
    "FEATURES.remove('BMI') # Highly correlated with BSA\n",
    "FEATURES.remove('Height') # Highly correlated with BSA\n",
    "FEATURES.remove('Ht') # Highly correlated with Hb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00bce3",
   "metadata": {},
   "source": [
    "# SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b571c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[FEATURES].to_csv('../dataset/X_train.csv')\n",
    "X_test[FEATURES].to_csv('../dataset/X_test.csv')\n",
    "y_train.to_csv('../dataset/y_train.csv')\n",
    "y_test.to_csv('../dataset/y_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

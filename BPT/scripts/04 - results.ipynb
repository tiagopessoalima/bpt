{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d53a5ac",
   "metadata": {},
   "source": [
    "# CONSTANTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79da22f",
   "metadata": {},
   "source": [
    "- **RANDOM_STATE** is a constant that is typically used to initialize the random number generator in a library or algorithm that uses randomness in some aspect of its operation. This means that when the RANDOM_STATE is fixed to a certain value, the algorithm will generate the same random results every time it is run, which can be useful for reproducing results and ensuring result consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2eab0",
   "metadata": {},
   "source": [
    "- **ALPHA** is a constant that is commonly used in statistical hypothesis testing to determine the significance level of a test. The value of ALPHA is typically set to 0.05 (or 5%), which means there is a 5% chance that the test results will be considered significant by chance. The value of ALPHA is used in conjunction with the p-value calculated from the data to determine whether the null hypothesis should be rejected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8109725",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a89e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, chi2_contingency, friedmanchisquare, shapiro, wilcoxon, kruskal\n",
    "\n",
    "from sklearn.model_selection import cross_validate, RepeatedStratifiedKFold\n",
    "from joblib import load\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c3d55",
   "metadata": {},
   "source": [
    "# READ FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59609371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../dataset/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../dataset/y_train.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda289fb",
   "metadata": {},
   "source": [
    "# READ MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b07c9",
   "metadata": {},
   "source": [
    "The code provided below performs the following tasks: It initializes an empty dictionary called \"models.\" Then, it sets a folder path where the models are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f646d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionary\n",
    "models = {}\n",
    "\n",
    "# Set the folder path\n",
    "folder_path = '../models/'\n",
    "\n",
    "# List all subdirectories in the folder\n",
    "subdirectories = [subdir for subdir in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subdir))]\n",
    "\n",
    "# Iterate over the subdirectories\n",
    "for subdir in subdirectories:\n",
    "    subdirectory_path = os.path.join(folder_path, subdir)\n",
    "    \n",
    "    # List all files in the subdirectory\n",
    "    files = os.listdir(subdirectory_path)\n",
    "    \n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        file_path = os.path.join(subdirectory_path, file)\n",
    "        \n",
    "        # Extract the file name without extension\n",
    "        model_name = os.path.splitext(file)[0]\n",
    "        \n",
    "        # Load the model using joblib.load and add it to the dictionary\n",
    "        models[model_name] = joblib.load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a50a6",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ed9d4",
   "metadata": {},
   "source": [
    "The code provided below implements a model evaluation process using cross-validation. It iterates through a list of model names and conducts cross-validation for each model. Depending on the model name, the training data is preprocessed accordingly, and the resulting metrics, such as accuracy, precision, recall, F1 score, and ROC AUC, are gathered for each model. These evaluation results are then stored in a dictionary named \"results.\" This code efficiently evaluates multiple models and provides a comprehensive understanding of their performance on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99685cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    \n",
    "    print(model_name)\n",
    "    \n",
    "    if model_name in ['TRACK','TRUST']:\n",
    "        X_train_model = X_train[model_name].values.reshape(-1, 1)\n",
    "    else:\n",
    "        X_train_model = X_train.drop(['TRACK','TRUST'],axis=1)\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        models[model_name], \n",
    "        X_train_model, \n",
    "        y_train.values.ravel(), \n",
    "        cv=RepeatedStratifiedKFold(random_state=RANDOM_STATE), \n",
    "        scoring=['accuracy','precision','recall','f1','roc_auc']\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'precision': cv_results['test_precision'],\n",
    "        'accuracy': cv_results['test_accuracy'],\n",
    "        'roc_auc': cv_results['test_roc_auc'],\n",
    "        'recall': cv_results['test_recall'],\n",
    "        'f1': cv_results['test_f1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2584b00",
   "metadata": {},
   "source": [
    "## - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905ceb5",
   "metadata": {},
   "source": [
    "A comprehensive summary of the model performance is provided in the form of a dataframe that includes the relevant performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of results for each model\n",
    "all_results = []\n",
    "for model_name in results:\n",
    "    # Creating a list of dictionaries containing the means and standard deviations of each metric for the current model\n",
    "    data = []\n",
    "    for metric in ['precision', 'accuracy', 'roc_auc', 'recall', 'f1']:\n",
    "        mean = results[model_name][metric].mean()\n",
    "        std = results[model_name][metric].std()\n",
    "        data.append({'Model': model_name, 'Metric': metric, 'Mean_Std': f'{mean:.4f} Â± {std:.4f}'})\n",
    "    \n",
    "    # Creating a DataFrame from the data for the current model\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Adding the DataFrame for the current model to the list of results\n",
    "    all_results.append(df)\n",
    "\n",
    "# Concatenating all the DataFrames into a single DataFrame\n",
    "results_df = pd.concat(all_results, axis=0)\n",
    "\n",
    "# Setting the index to the name of the model\n",
    "results_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Reorganizing the DataFrame\n",
    "results_df = results_df.pivot(columns='Metric', values='Mean_Std')\n",
    "\n",
    "# Displaying the DataFrame with the metrics in the desired order\n",
    "results_df.loc[:,['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f696215",
   "metadata": {},
   "source": [
    "- **Accuracy:** It measures the percentage of correct predictions made out of all predictions. It is defined as the ratio of the number of correct predictions to the total number of predictions made. It is commonly used as an evaluation metric when the number of positive and negative samples is roughly the same.\n",
    "\n",
    "- **Precision:** It measures the percentage of true positives (correctly identified positives) out of all positive predictions made. It is defined as the ratio of true positives to true positives plus false positives. It is useful when the cost of false positives is high.\n",
    "\n",
    "- **Recall** (Sensitivity or True Positive Rate): It measures the percentage of true positives identified out of all actual positive samples. It is defined as the ratio of true positives to true positives plus false negatives. It is useful when the cost of false negatives is high.\n",
    "\n",
    "- **F1:** It is a harmonic mean of precision and recall. It is defined as the weighted average of precision and recall, where the weights are the same, i.e., `F1 = 2 * (precision * recall) / (precision + recall)`. It is a useful metric when both precision and recall are important.\n",
    "\n",
    "- **ROC AUC** (Receiver Operating Characteristic Area Under the Curve): It measures the ability of a model to distinguish between positive and negative classes. It is calculated by plotting the true positive rate against the false positive rate for different thresholds and calculating the area under the curve. A perfect model would have an ROC AUC of 1.0, while a model that performs no better than random guessing would have an ROC AUC of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73bf31",
   "metadata": {},
   "source": [
    "## - Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614441ac",
   "metadata": {},
   "source": [
    "A boxplot is a powerful tool for visualizing the distribution of a dataset. The box of the plot represents the interquartile range (IQR), which contains 50% of the data, and the whiskers extend to the minimum and maximum values, excluding outliers. Outliers are represented as points outside the whiskers. The median, which is the middle value of the dataset, is shown as a horizontal line within the box. Boxplots can be used to compare the distributions of multiple groups, identify skewness or symmetry, and detect any unusual observations that might need further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(metric):\n",
    "    scores = [results[model][metric] for model in models]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(scores,\n",
    "               boxprops={'linewidth': 2, 'color': 'blue'},\n",
    "               whiskerprops={'linewidth': 2, 'color': 'blue'},\n",
    "               medianprops={'linewidth': 2, 'color': 'red'})\n",
    "    ax.set_xticklabels(models, fontsize=10, rotation='vertical')\n",
    "    ax.set_ylabel(metric.capitalize(), fontsize=10)\n",
    "    ax.set_title('Results', fontsize=12)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995f66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62798843",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot('roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4351f2",
   "metadata": {},
   "source": [
    "The AUC (Area Under the Curve) is a common metric for evaluating binary classification models, such as predicting whether a patient will survive or not. The AUC measures the model's ability to correctly classify positive (e.g., patients who died) and negative (e.g., patients who survived) examples, and is especially useful when the class distribution is imbalanced. On the other hand, the f1-score is a performance measure that considers both precision and recall of the model. Precision measures the proportion of positive examples correctly classified among all predicted positive examples, while recall measures the proportion of positive examples correctly classified among all actual positive examples. The f1-score is a harmonic mean between precision and recall and can be useful when the balance between precision and recall is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3f8de",
   "metadata": {},
   "source": [
    "## - Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c8fd8",
   "metadata": {},
   "source": [
    "Confidence interval is a statistical measure that indicates the range of likely values for an estimate with a certain degree of confidence. It is commonly used in statistical inference to provide a measure of uncertainty around a point estimate. The confidence interval provides a range of plausible values for the population parameter based on a sample of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b285892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(model_name, results, metric='roc_auc', confidence_level=0.95, decimal_place=4):\n",
    "\n",
    "    results = results[model_name][metric]\n",
    "    n = len(results)\n",
    "    mean, sem, std = np.mean(results), stats.sem(results), np.std(results)\n",
    "\n",
    "    confidence_interval = tuple(map(lambda x: round(x, decimal_place), stats.t.interval(confidence_level, n-1, mean, sem)))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Model Name': model_name,\n",
    "        'Metric': metric,\n",
    "        'Point Estimate': round(mean, decimal_place),\n",
    "        'Standard Deviation': round(std, decimal_place),\n",
    "        'Lower CI': confidence_interval[0],\n",
    "        'Upper CI': confidence_interval[1],\n",
    "        'Confidence Level': confidence_level,\n",
    "        'Sample Size': n\n",
    "    }, index=[0])\n",
    "\n",
    "confidence_interval = pd.concat([mean_confidence_interval(model_name, results, metric) for metric in results_df.columns for model_name in models]).reset_index(drop=True)\n",
    "\n",
    "# Set the metric as index\n",
    "confidence_interval.set_index('Metric', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bed954",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval.loc['accuracy'].sort_values(by='Point Estimate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa93376",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval.loc['precision'].sort_values(by='Point Estimate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35219cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval.loc['recall'].sort_values(by='Point Estimate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa21e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval.loc['f1'].sort_values(by='Point Estimate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval.loc['roc_auc'].sort_values(by='Point Estimate', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac938d",
   "metadata": {},
   "source": [
    "# STATISTICAL TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e272ea7",
   "metadata": {},
   "source": [
    "## Normality Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643a7ca",
   "metadata": {},
   "source": [
    "The normality test is important because many statistical methods assume that data are normally distributed. If a distribution is not normal, these methods may not be appropriate or require adjustments to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(results, metric):\n",
    "    normality_dict = {}\n",
    "    for model_name in models:\n",
    "        score_values = results[model_name][metric]\n",
    "        stat, p = shapiro(score_values)\n",
    "        normality_dict[model_name] = {'Statistic': stat, 'p-value': p, 'Normality': p < ALPHA}\n",
    "    \n",
    "    return pd.DataFrame.from_dict(normality_dict, orient='index').assign(Metric=metric)\n",
    "\n",
    "normality_test = pd.concat([normality_test(results, metric) for metric in results_df.columns], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test.query('Metric == \"accuracy\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test.query('Metric == \"precision\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96565aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test.query('Metric == \"recall\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test.query('Metric == \"f1\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_test.query('Metric == \"roc_auc\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e03ca",
   "metadata": {},
   "source": [
    "## Friedman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53160628",
   "metadata": {},
   "source": [
    "The Friedman test is used when the data are paired, such as when comparing the same set of subjects under different conditions. It tests the null hypothesis that the groups have the same distribution, and the alternative hypothesis that at least one group has a different distribution. The test statistic is calculated by ranking the data within each group and summing the ranks for each group. The test statistic is then compared to a chi-squared distribution with degrees of freedom equal to the number of groups minus one. If the p-value is less than the chosen significance level, then the null hypothesis is rejected, and it can be concluded that there is a significant difference between the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scores\n",
    "scores = [results[key][metric] for key in results.keys() for metric in results_df.columns]\n",
    "\n",
    "# Create a dictionary of statistics\n",
    "data = {'metric': [], 'statistic': [], 'p-value': [], 'significant difference': []}\n",
    "\n",
    "# Loop through the metrics\n",
    "for metric in results_df.columns:\n",
    "    metric_scores = scores[:len(results.keys())]\n",
    "    scores = scores[len(results.keys()):]\n",
    "    stat, p_value = friedmanchisquare(*metric_scores)\n",
    "    data['metric'].append(metric)\n",
    "    data['statistic'].append(stat)\n",
    "    data['p-value'].append(p_value)\n",
    "    if p_value < ALPHA:\n",
    "        data['significant difference'].append('Yes')\n",
    "    else:\n",
    "        data['significant difference'].append('No')\n",
    "\n",
    "# Create the dataframe\n",
    "friedman = pd.DataFrame(data)\n",
    "\n",
    "# Set the metric as index\n",
    "friedman.set_index('metric', inplace=True)\n",
    "\n",
    "# Show the dataframe\n",
    "friedman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7817934",
   "metadata": {},
   "source": [
    "For all the metrics (accuracy, f1, precision, recall, and roc_auc), the p-value is less than the commonly adopted significance level of 0.05. Therefore, we can assert that there are statistically significant differences between the samples for all these metrics.\n",
    "\n",
    "The fact that the p-value is very close to zero provides strong evidence against the null hypothesis that there are no differences between the samples. This suggests that at least one of the samples is significantly different from the others in terms of each evaluated metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf483993",
   "metadata": {},
   "source": [
    "## Wilcoxon-Mann-Whitney"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b003413",
   "metadata": {},
   "source": [
    "The Wilcoxon-Mann-Whitney test is a nonparametric test used to determine if two independent samples were drawn from populations with the same distribution. It is used as an alternative to the two-sample t-test when the assumptions of normality and equal variances are not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wilcoxon_heatmap(metric):\n",
    "\n",
    "    MODEL_NAMES = list(models.keys())\n",
    "\n",
    "    n_models = len(MODEL_NAMES)\n",
    "\n",
    "    # Calculate the p-value of the Wilcoxon test for each pair of classifiers\n",
    "    wilcoxon_matrix = np.zeros((n_models, n_models))\n",
    "    for i, clf1 in enumerate(MODEL_NAMES):\n",
    "        for j, clf2 in enumerate(MODEL_NAMES[i+1:], i+1):\n",
    "            _, p_value = wilcoxon(results[clf1][metric], results[clf2][metric])\n",
    "            wilcoxon_matrix[i, j] = p_value\n",
    "\n",
    "    # Plot the matrix using a color scale\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(wilcoxon_matrix, cmap='coolwarm', vmin=0, vmax=1)\n",
    "\n",
    "    # Add classifier labels\n",
    "    ax.set_xticks(range(n_models))\n",
    "    ax.set_yticks(range(n_models))\n",
    "    ax.set_xticklabels(MODEL_NAMES, fontsize=10, rotation=90)\n",
    "    ax.set_yticklabels(MODEL_NAMES, fontsize=10)\n",
    "\n",
    "    # Add p-values to each cell\n",
    "    for i in range(n_models):\n",
    "        for j in range(i+1, n_models):\n",
    "            p_value = wilcoxon_matrix[i, j]\n",
    "            text_color = 'white' if p_value < ALPHA else 'black'\n",
    "            text = ax.text(j, i, '{:.2f}'.format(p_value), ha='center', va='center', color=text_color, fontsize=10, fontweight='bold' if p_value < ALPHA else 'normal')\n",
    "\n",
    "    # Configure color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel('p-value', rotation=-90, va='bottom', fontsize=10)\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Wilcoxon Test - {}'.format(metric), fontsize=12)\n",
    "\n",
    "    # Display plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb63edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wilcoxon_heatmap(metric='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wilcoxon_heatmap('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9283ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wilcoxon_heatmap('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wilcoxon_heatmap('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wilcoxon_heatmap('roc_auc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
